{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Example of training nanoGPT</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src import config\n",
    "from src.data.dataset import NextTokenDataset\n",
    "from src.data.tokenizer import CharTokenizer\n",
    "from src.model.gpt_language_model.gpt import GPTLanguageModel\n",
    "from src.model.trainer import Trainer\n",
    "from src.utils.device import get_device\n",
    "from src.utils.seed import set_seed\n",
    "from src.utils.arguments import grab_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if debug is true then a small gpt will be trained, if not - a large one\n",
    "DEBUG = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple model we will be using rather simple tiny shakespeare dataset, it consists of over 1 million of characters and the size is slightly over 1 Mb of disk space, so it's quite small. But the task for this repo is not to train the perfect language model for learning purposes, so this one should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parents[1] / config.datasets.tiny_shakespeare.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, \"r\") as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the text consists of quote blocks with the name of the actor and his replica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model cannot work with characters, so we have to transform set of characters into a set of indices, where each index tells the position of the characters in the vocabulary.\n",
    "\n",
    "The input 'abc' will be transformed into [1, 2, 3], given that we have vocabulary {'a': 1, 'b': 2, 'c': 3}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing mapping of the 10 first characters.\n",
      "F -> 18\n",
      "i -> 47\n",
      "r -> 56\n",
      "s -> 57\n",
      "t -> 58\n",
      "  -> 1\n",
      "C -> 15\n",
      "i -> 47\n",
      "t -> 58\n",
      "i -> 47\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharTokenizer(corpus=text)\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"Printing mapping of the 10 first characters.\")\n",
    "for idx in range(10):\n",
    "    print(f\"{text[idx]} -> {data[idx]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the data into two parts: train and test. The train part will be used during training, while test - during evaluation. Evaluation allows us to see how good the trained model predicts on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% for the training, 10% - fot the evaluating\n",
    "test_split = int(len(data) * config.dataloader.test_split)\n",
    "train_data, test_data = data[:test_split], data[test_split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader creates batches of tuples of the data, where the first element in the tuple is inputs, while the second - targets. Both are needed for the training and evaluating steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config.model.gpt.size.small\n",
    "context_size = model_config.context_size\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "# dataset class creates pairs (inputs, targets)\n",
    "train_dataset = NextTokenDataset(train_data, context_size)\n",
    "test_dataset = NextTokenDataset(test_data, context_size)\n",
    "\n",
    "# dataloader creates batches of pairs efficiently\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config.model.gpt.size.small if DEBUG else config.model.gpt.size.large\n",
    "context_size = model_config.context_size\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "# dataset class creates pairs (inputs, targets)\n",
    "train_dataset = NextTokenDataset(train_data, context_size)\n",
    "test_dataset = NextTokenDataset(test_data, context_size)\n",
    "\n",
    "# dataloader creates batches of pairs efficiently\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=config.dataloader.num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=config.dataloader.num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty straight forward: use trainer (contains logic for the training and evaluation) and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 21:37:53.078 | DEBUG    | src.model.trainer:train:74 - Training on 'cpu' device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Epoch: 0 ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|##########| 31371/31371 [05:00<00:00, 104.42it/s, loss=1.92]\n",
      "eval: 100%|##########| 3486/3486 [00:11<00:00, 314.07it/s, loss=2.17]\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **grab_arguments(GPTLanguageModel, model_config),\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=model_config.learning_rate)\n",
    "trainer = Trainer(model, optimizer, train_dataloader, test_dataloader, get_device())\n",
    "trainer.train(epochs=model_config.epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate new characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have trained model we can use it to create new characters. \n",
    "\n",
    "All we need is to provide context and the model will try to continue the text. If we provide tensor with zeros we basically do not proved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ThATHARICHIH:\n",
      "Whem bemer, we him whou the sill Here not stiscoy, thep she bair.\n",
      "\n",
      "KANIO:\n",
      "I der, 'mak \n"
     ]
    }
   ],
   "source": [
    "def generate_text(context: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(model.generate(context, max_new_tokens=100).squeeze().tolist())\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(generate_text(context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can provide first 10 characters as context, but in this case, for the simple bigram model, will not make any difference, as such model doesn't care about the context. But it will work with more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citio, sill?\n",
      "You the you pevens you knot hear liths leace at cabe to his give, strountle ret; sbumaintor\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(tokenizer.encode(text[:10])).unsqueeze(dim=0)\n",
    "print(generate_text(context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6de188a6508f9f350a997ad6df62530f7a91cb3c4106b36ebaa288db005407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
